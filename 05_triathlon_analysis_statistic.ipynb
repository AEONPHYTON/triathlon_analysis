{"cells":[{"cell_type":"markdown","metadata":{"id":"buCIrfuG7VGK"},"source":["# Create a classical statistic and machine learning for triathlon analysis database (find the difference)\n","## The pourpose of this analysis is:\n","- Find the differente from top 3 athlete and average 3 best perfomance in every discipline\n","  - There's a difference from top 3 and avg3?\n","  - The top 3 are the best athlete compared to the avg 3?\n","- Find the difference between top 3 and other position\n","  - Top 3 athletes is the best perfoprmer in every discipline?\n","  - There's a difference from top 3 and other position?\n","  - What is the pposition in the discipline for the top 3 athletes?\n","- Find the most influenced part of the race (excluded the transition)\n","  - Using different model of analysis, the result is the same?\n","  - using statistic and machine learning, qhat is the most influenced part of the race?\n","  - What is the second and third part of the race most influenced, and the difference betweet them\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97660,"status":"ok","timestamp":1723708423079,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"IQPYdYSMdLxc","outputId":"3cfb8e20-94c6-489b-bbee-f5d18823bf17"},"outputs":[],"source":["# install Thorch use this only for a coolab or jupyter notebook environment\n","# !pip install torch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20025,"status":"ok","timestamp":1723708443093,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"3gLf2xed7Gvo"},"outputs":[],"source":["# import libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","import numpy as np\n","import seaborn as sns\n","from scipy.stats import ttest_ind\n","from IPython.display import display, Math\n","from sklearn.metrics import r2_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression\n","from scipy.stats import friedmanchisquare\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, accuracy_score\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","import itertools\n","from scipy.stats import ttest_ind\n"]},{"cell_type":"markdown","metadata":{"id":"iW6F3bWuWDhS"},"source":["To set the correct gender, uncomment the code below"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723708443093,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"hVy8dFAIVSHL"},"outputs":[],"source":["# upload dataset (standard female o male) uncomment/comment\n","\n","#train_data_path = \"/content/drive/MyDrive/TRIATHLON ANALYSIS/db_standard_female_final.csv\"\n","train_data_path = \"/content/drive/MyDrive/TRIATHLON ANALYSIS/db_standard_male_final.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":523},"executionInfo":{"elapsed":635,"status":"ok","timestamp":1723708443726,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"iPu5Cwos7mEY","outputId":"67b01fe5-3d51-4028-e6e3-f3071ac5c2e6"},"outputs":[],"source":["train_df = pd.read_csv(train_data_path, low_memory=False) # open file\n","train_df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708443726,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"N0M_ZdCuS8FK","outputId":"e14c1729-6876-47aa-c0ff-65c0607f87f8"},"outputs":[],"source":["train_df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1723708444086,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"MxurtouQk46c","outputId":"e0a16407-ed8e-4c04-f637-3ac084a4db45"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"tTrkIvjj9Ytn"},"source":["Visualize dataset in a scatter chart to undestand the final position and the time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":602},"executionInfo":{"elapsed":2514,"status":"ok","timestamp":1723708446596,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"GuJ3pTc09X1v","outputId":"0fb174bc-2ee3-44af-b7fb-92e4dd3ed86e"},"outputs":[],"source":["# Convert 'event_date' in datetime\n","train_df['event_date'] = pd.to_datetime(train_df['event_date'])\n","\n","# sort DataFrame previously for 'event_date' and then for 'position'\n","df_sorted = train_df.sort_values(by=['event_date', 'position'])\n","\n","# make a scatter plot\n","plt.figure(figsize=(15, 6))\n","\n","years = pd.date_range(start=df_sorted['event_date'].min(),\n","                      end=df_sorted['event_date'].max(),\n","                      freq='YS')\n","\n","plt.xticks(years.to_pydatetime(), years.year, rotation=90)\n","\n","plt.scatter(df_sorted['event_date'], df_sorted['position'], alpha=0.6)\n","plt.title('Race Positions Over Time')\n","plt.xlabel('Event Date')\n","plt.ylabel('Position')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"n8rqTnk5A2ac"},"source":["During the data cleaning in the previous notebook, some race are cutted for the position. a time below 2500 seconds are deleted from the dataset\n","In this chart we visualize the race with no firs, second or third position\n","- delete race before 1989/01/01\n","- delete all the race without the first, second and third position"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1723708446986,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"4LHexRkMioGi","outputId":"2e3cfe5c-d0d5-479a-f200-be519eaef3ee"},"outputs":[],"source":["# Specify the cutoff date\n","cutoff_date = '1989-01-01'\n","\n","# Filter the DataFrame to remove rows before the cutoff date\n","df_sorted = df_sorted.loc[df_sorted['event_date'] >= cutoff_date]\n","df_sorted.reset_index(drop=True, inplace=True)\n","df_sorted"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708446986,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"DUnN2IvOxqrD","outputId":"23f3573f-b98b-4ee3-ecb3-c9203ce204b7"},"outputs":[],"source":["# Some information\n","length = len(df_sorted)\n","unique_race = len(df_sorted[\"event_date\"].unique())\n","race_date = df_sorted[\"event_date\"].unique().strftime('%Y-%m-%d')\n","\n","print(f\"Total row = {length}\")\n","print(f\"Total race is = {unique_race}\")\n","print(f\"Race date from {race_date[0]} to {race_date[-1]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723708446986,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"G9jmYIRG7_QG","outputId":"c2301993-6e5a-4b03-d76f-ed6a53f2e708"},"outputs":[],"source":["# Group by year and event_date, then count the occurrences\n","df_sorted['date_of_event'] = df_sorted['date_of_event'].astype(int)\n","date_counts = df_sorted.groupby(['date_of_event', 'event_date']).size().reset_index(name='count')\n","\n","# Filter to only keep dates that are repeated (i.e., count > 1)\n","repeated_dates = date_counts[date_counts['count'] > 1]\n","\n","# Count the number of repeated dates for each year\n","repeated_dates_per_year = repeated_dates.groupby('date_of_event').size()\n","\n","# Display the result\n","print(repeated_dates_per_year)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":368,"status":"error","timestamp":1723719805182,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"RSHgZm-C1fBY","outputId":"13b07351-fa9f-40f6-ad05-56e9669f1091"},"outputs":[],"source":["# plot data\n","file_female= \"/content/drive/MyDrive/TRIATHLON ANALYSIS/db_standard_female_final.csv\"\n","if train_data_path == file_female:\n","  sex = \"(Female)\"\n","else:\n","  sex = \"(Male)\"\n","\n","\n","plt.figure(figsize=(10, 6))\n","\n","repeated_dates_per_year.plot(kind='line', marker='.', color=\"#626262\", alpha=0.7)\n","plt.title(f\"Number of Event per Year {sex}\")\n","plt.xlabel('Year')\n","plt.ylabel('Number of Events')\n","plt.xticks(ticks=repeated_dates_per_year.index, labels=repeated_dates_per_year.index, rotation=90)\n","plt.grid(True)\n","\n","for x, y in zip(repeated_dates_per_year.index, repeated_dates_per_year):\n","    plt.text(int(x), y, str(y), ha=\"left\", va=\"bottom\", fontsize=11)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"executionInfo":{"elapsed":2393,"status":"ok","timestamp":1723708450534,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"y6YihIqhlBor","outputId":"c053df67-502d-4a95-dad7-745e345f59fd"},"outputs":[],"source":["# make a scatter plot to conferm the cutting date\n","plt.figure(figsize=(15, 6))\n","\n","years = pd.date_range(start=df_sorted['event_date'].min(),\n","                      end=df_sorted['event_date'].max(),\n","                      freq='YS')\n","\n","plt.xticks(years.to_pydatetime(), years.year, rotation=90)\n","\n","plt.scatter(df_sorted['event_date'], df_sorted['position'], alpha=0.3)\n","plt.title('Race Positions Over Time')\n","plt.xlabel('Event Date')\n","plt.ylabel('Position')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1723708450882,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"pq3waSpARhFU","outputId":"11f1f6b1-0162-439e-fdfc-a02d16c58479"},"outputs":[],"source":["\n","def filter_races_with_positions(df, position_col='position', event_col='event_date'):\n","    # list of event to keep (position 1, 2, 3 in a list)\n","    valid_events = []\n","\n","    # group for 'event_date'\n","    for date, group in df.groupby(event_col):\n","        # chech if all the positions are present in the group\n","        if all(pos in group[position_col].values for pos in [1, 2, 3]):\n","            valid_events.append(date)\n","\n","    # Filter the DataFrame to keep only the rows with valid events\n","    filtered_df = df[df[event_col].isin(valid_events)]\n","\n","    return filtered_df\n","\n","# aplly the function on DataFrame\n","df_filtered = filter_races_with_positions(df_sorted)\n","\n","# check the number of events\n","print(f\"Numero di eventi originali: {df_sorted['event_date'].nunique()}\")\n","print(f\"Numero di eventi dopo il filtro: {df_filtered['event_date'].nunique()}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":691},"executionInfo":{"elapsed":3738,"status":"ok","timestamp":1723708454618,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"2XMusiP_AqbO","outputId":"e5458595-941e-4bc3-a8ec-e8134a4cad38"},"outputs":[],"source":["# Plot the filtered data\n","plt.figure(figsize=(15, 8))\n","\n","# chart of the filtered DataFrame\n","plt.scatter(df_filtered['event_date'], df_filtered['position'], color='gray', label='All Positions', alpha=0.5)\n","\n","# set the x and y axis\n","years = pd.date_range(start=df_filtered['event_date'].min(),\n","                      end=df_filtered['event_date'].max(),\n","                      freq='YS')\n","\n","plt.xticks(years.to_pydatetime(), years.year, rotation=90)\n","\n","plt.title(f'Race Positions Over Time (Filtered for Positions 1, 2, 3) {sex}')\n","plt.xlabel('Event Date')\n","plt.ylabel('Position')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"5pVPoY3PGiWA"},"source":["To have correct data also in the test dataset, visualize the position and the date for the test dataset"]},{"cell_type":"markdown","metadata":{"id":"pE7Ub4o9L7vR"},"source":["We need to check if the data have not duplicates or null row for train and test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723708454618,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"sRTfjv2w7mBv","outputId":"710561cc-a61d-4882-a8f5-2ea03313dd91"},"outputs":[],"source":["# Check for duplicates for train dataset\n","duplicates = train_df.duplicated()\n","if duplicates.any():\n","    print(\"Found duplicates. Removing...\")\n","    # Remove duplicates\n","    df_final_train = train_df.drop_duplicates()\n","    print(\"Duplicates removed.\")\n","else:\n","    print(\"No duplicates found.\")\n","\n","# Check for missing data in any row\n","missing_data = train_df.isnull().any(axis=1)\n","if missing_data.any():\n","    print(f\"Found rows with missing data. Removing {missing_data.sum()} rows...\")\n","    # Remove rows with missing data\n","    train_df = train_df.dropna()\n","    print(\"Rows with missing data removed.\")\n","else:\n","    print(\"No missing data found.\")"]},{"cell_type":"markdown","metadata":{"id":"2OHYXvVoNCxY"},"source":["# Make a standard statistic analysis\n","The analysis have different pourpose:\n","\n","1. Find average time of the top 3 athlete\n","2. Discorver the correlation between race\n","3. Find the performance of the top 3 athlete\n","4. There's a correltion if I use teh top 3 final position or user the top 3 position in swmi, bike and run?\n","5. Using a simple linear regression, can I determine the total time of the race in Paris 2024?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oljbI-kn3S2X"},"source":["Create 2 dataframe\n","1. With top 3 athlete (the 1st, 2nd, and 3rd place of the race)\n","2. with the best 3 time in swim, bike, run and total time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11289,"status":"ok","timestamp":1723708465905,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"NY9i3JEDPYGj","outputId":"3ebeb22d-fdc8-41a2-b643-b01c2c6a3a18"},"outputs":[],"source":["# function to create a top3 dataframe\n","def create_top3_structure(df, event_col, swim_col, bike_col, run_col, total_time_col, position_col):\n","    # filtering fot the top 3 positions for each event\n","    top3_df = df[df[position_col] <= 3].sort_values(by=[event_col, position_col]).groupby(event_col).head(3)\n","\n","    # select the columns that you need:\n","    top3_df = top3_df[[event_col, swim_col, bike_col, run_col, total_time_col]]\n","\n","    return top3_df\n","\n","# function to create a avg3 dataframe with the same structure\n","def create_avg3_structure(df, event_col, swim_col, bike_col, run_col, total_time_col):\n","    avg3_df = pd.DataFrame(columns=[event_col, swim_col, bike_col, run_col, total_time_col])\n","\n","    for event in df[event_col].unique():\n","        event_data = df[df[event_col] == event]\n","\n","        # take the 3 best times for each discipline\n","        swim_times = event_data[swim_col].nsmallest(3).values\n","        bike_times = event_data[bike_col].nsmallest(3).values\n","        run_times = event_data[run_col].nsmallest(3).values\n","        total_times = event_data[total_time_col].nsmallest(3).values\n","\n","        # only if there are 3 values in the list\n","        if len(swim_times) == 3 and len(bike_times) == 3 and len(run_times) == 3 and len(total_times) == 3:\n","            event_df = pd.DataFrame({\n","                event_col: [event] * 3,\n","                swim_col: swim_times,\n","                bike_col: bike_times,\n","                run_col: run_times,\n","                total_time_col: total_times\n","            })\n","\n","            avg3_df = pd.concat([avg3_df, event_df], ignore_index=True)\n","\n","    return avg3_df\n","\n","# apply the function\n","train_df_top3 = create_top3_structure(df_filtered, 'event_date', 'swim_time', 'bike_time', 'run_time', 'total_time', 'position')\n","train_df_avg3 = create_avg3_structure(df_filtered, 'event_date', 'swim_time', 'bike_time', 'run_time', 'total_time')\n","\n","# check the length of the top3 and avg3\n","print(f\"Lunghezza top3: {len(train_df_top3)}\")\n","print(f\"Lunghezza avg3: {len(train_df_avg3)}\")\n","print(f\"Colonne top3: {train_df_top3.columns.tolist()}\")\n","print(f\"Colonne avg3: {train_df_avg3.columns.tolist()}\")\n","\n","# show the top3 and avg3 header\n","print(train_df_top3.head())\n","print(train_df_avg3.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708465905,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"yuWqjm9kOFNc","outputId":"7e5e8ba6-bf21-485e-cc45-c7a882759cce"},"outputs":[],"source":["# check the length of the top3 and avg3\n","len_top3 = len(train_df_top3)\n","len_avg3 = len(train_df_avg3)\n","\n","# compare the lengths\n","if len_top3 == len_avg3:\n","    print(f\"The two DataSet have the same length: {len_top3} rows.\")\n","else:\n","    print(f\"The dataset have different lengths. top3: {len_top3} rows, avg3: {len_avg3} rows.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723708465906,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"QkCA5gq63IJ2"},"outputs":[],"source":["# big_chart function to use any dataframe passed as an argument\n","\n","def big_chart(df, df_name): #df = name of the dataframe, df_name = string with the name of the dataframe\n","\n","    # Convert date to datetime if not already done\n","    df.loc[:, 'event_date'] = pd.to_datetime(df['event_date'])\n","\n","    years = pd.date_range(start=df['event_date'].min(),\n","                          end=df['event_date'].max(),\n","                          freq='YS')\n","\n","    # Calculate mean and standard deviation\n","    mean_swim = df['swim_time'].mean()\n","    std_swim = df['swim_time'].std() * 2\n","\n","    mean_bike = df['bike_time'].mean()\n","    std_bike = df['bike_time'].std() * 2\n","\n","    mean_run = df['run_time'].mean()\n","    std_run = df['run_time'].std() * 2\n","\n","    mean_total = df['total_time'].mean()\n","    std_total = df['total_time'].std() * 2\n","\n","    # Make a chart for the data\n","    plt.figure(figsize=(20, 12))\n","\n","    plt.subplot(4, 1, 1)\n","    plt.scatter(df['event_date'], df['swim_time'], label='Swim Time', color='blue', alpha=0.3)\n","    plt.axhline(mean_swim, color='orange', linestyle='dashed', linewidth=2, label='Mean')\n","    plt.axhline(mean_swim + std_swim, color='red', linestyle='dotted', linewidth=2, label='Std Dev')\n","    plt.axhline(mean_swim - std_swim, color='red', linestyle='dotted', linewidth=2)\n","    plt.xlabel('Date')\n","    plt.ylabel('Time (seconds)')\n","    plt.title(f'{df_name} Swim Time {sex}')\n","    plt.xticks(years.to_pydatetime(), years.year, rotation=45)\n","    #plt.ylim(0, 2000)\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.subplot(4, 1, 2)\n","    plt.scatter(df['event_date'], df['bike_time'], label='Bike Time', color='green', alpha=0.3)\n","    plt.axhline(mean_bike, color='orange', linestyle='dashed', linewidth=2, label='Mean')\n","    plt.axhline(mean_bike + std_bike, color='red', linestyle='dotted', linewidth=2, label='Std Dev')\n","    plt.axhline(mean_bike - std_bike, color='red', linestyle='dotted', linewidth=2)\n","    plt.xlabel('Date')\n","    plt.ylabel('Time (seconds)')\n","    plt.title(f'{df_name} Bike Time {sex}')\n","    plt.xticks(years.to_pydatetime(), years.year, rotation=45)\n","    #plt.ylim(0, 6000)\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.subplot(4, 1, 3)\n","    plt.scatter(df['event_date'], df['run_time'], label='Run Time', color='red', alpha=0.3)\n","    plt.axhline(mean_run, color='orange', linestyle='dashed', linewidth=2, label='Mean')\n","    plt.axhline(mean_run + std_run, color='red', linestyle='dotted', linewidth=2, label='Std Dev')\n","    plt.axhline(mean_run - std_run, color='red', linestyle='dotted', linewidth=2)\n","    plt.xlabel('Date')\n","    plt.ylabel('Time (seconds)')\n","    plt.title(f'{df_name} Run Time {sex}')\n","    plt.xticks(years.to_pydatetime(), years.year, rotation=45)\n","    #plt.ylim(0, 3600)\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.subplot(4, 1, 4)\n","    plt.scatter(df['event_date'], df['total_time'], label='Total Time', color='purple', alpha=0.3)\n","    plt.axhline(mean_total, color='orange', linestyle='dashed', linewidth=2, label='Mean')\n","    plt.axhline(mean_total + std_total, color='red', linestyle='dotted', linewidth=2, label='Std Dev')\n","    plt.axhline(mean_total - std_total, color='red', linestyle='dotted', linewidth=2)\n","    plt.xlabel('Date')\n","    plt.ylabel('Time (seconds)')\n","    plt.title(f'{df_name} Total Time {sex}')\n","    plt.xticks(years.to_pydatetime(), years.year, rotation=45)\n","    #plt.ylim(3600, 10000)\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Use the big_chart function with different dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":707},"executionInfo":{"elapsed":7643,"status":"ok","timestamp":1723708473545,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"WM9J15tM895g","outputId":"a88a3a01-dcf1-4937-894f-702b33de81a9"},"outputs":[],"source":["big_chart(train_df_top3, \"train_df_top3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":707},"executionInfo":{"elapsed":5968,"status":"ok","timestamp":1723708479501,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"tmsKhPUh3IHn","outputId":"fce07d9f-01b4-473b-fc01-8a571f81208e"},"outputs":[],"source":["big_chart(train_df_avg3, \"train_df_avg3\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708479501,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"gKiVJVH43IFh"},"outputs":[],"source":["# calculate the correlation\n","\n","def calcola_correlazione(df1, df2):\n","    corr = df1[['swim_time', 'bike_time', 'run_time', 'total_time']].corrwith(df2[['swim_time', 'bike_time', 'run_time', 'total_time']])\n","    return corr"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708479502,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"267OqdbN3IDW"},"outputs":[],"source":["# calculate the t test\n","\n","def calcola_ttest_latex(df1, df2):\n","    ttest_results = {}\n","    for col in ['swim_time', 'bike_time', 'run_time', 'total_time']:\n","        t_stat, p_value = ttest_ind(df1[col], df2[col], equal_var=False)\n","        ttest_results[col] = {'t_statistic': t_stat, 'p_value': p_value}\n","\n","        # LaTeX formula\n","        formula_latex = f\"For {col}: \\, T = {t_stat:.4f}, \\, P = {p_value:.12f}\"\n","        display(Math(formula_latex))\n","\n","        if p_value < 0.01:\n","            print(\"Interpretation: Very Good - There is a highly significant difference between the disciplines.\")\n","        elif 0.01 <= p_value < 0.05:\n","            print(\"Interpretation: Good - There is a significant difference between the disciplines.\")\n","        elif 0.05 <= p_value < 0.10:\n","            print(\"Interpretation: Moderate - There is a marginal difference between the disciplines.\")\n","        elif 0.10 <= p_value < 0.20:\n","            print(\"Interpretation: Poor - There is little to no significant difference between the disciplines.\")\n","        else:\n","            print(\"Interpretation: Very Poor - There is no significant difference between the disciplines.\")\n","\n","    return ttest_results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708479502,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"25XTQ8faFL5M"},"outputs":[],"source":["def allinea_dataframes(df1, df2, on_column='event_date'):\n","    merged_df = pd.merge(df1, df2, on=on_column, suffixes=('_avg3', '_top3'))\n","    return merged_df"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708479502,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"IKc8pmNd3IBH"},"outputs":[],"source":["#calculate the R^2\n","\n","def calcola_rquadro_latex(df1, df2):\n","    merged_df = allinea_dataframes(df1, df2)\n","    r2_results = {}\n","    for col in ['swim_time', 'bike_time', 'run_time', 'total_time']:\n","        r2 = r2_score(merged_df[f'{col}_avg3'], merged_df[f'{col}_top3'])\n","        r2_results[col] = r2\n","\n","        # LaTeX formula\n","        formula_latex = f\"R^2 \\\\text{{ for }} {col} = {r2:.4f}\"\n","        display(Math(formula_latex))\n","\n","    return r2_results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708479502,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"sYdg2rKv3H-n"},"outputs":[],"source":["# create heatmap of the correlation\n","\n","def plot_heatmap_correlazione(df1, df2):\n","    combined_df = pd.concat([df1[['swim_time', 'bike_time', 'run_time', 'total_time']],\n","                             df2[['swim_time', 'bike_time', 'run_time', 'total_time']]],\n","                            axis=1)\n","    combined_df.columns = ['swim_time_avg3', 'bike_time_avg3', 'run_time_avg3', 'total_time_avg3',\n","                           'swim_time_top3', 'bike_time_top3', 'run_time_top3', 'total_time_top3']\n","    corr = combined_df.corr()\n","    sns.heatmap(corr, annot=True, cmap='coolwarm')\n","    plt.title(f\"Heat Map correlation between two dataset {sex}\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723708479502,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"ClfyEDeED_mq"},"outputs":[],"source":["# make linear regression\n","\n","def regressione_lineare(df1, df2):\n","    merged_df = allinea_dataframes(df1, df2)\n","    results = {}\n","    for col in ['swim_time', 'bike_time', 'run_time', 'total_time']:\n","        model = LinearRegression()\n","        X = merged_df[[f'{col}_avg3']].values.reshape(-1, 1)\n","        y = merged_df[f'{col}_top3'].values\n","        model.fit(X, y)\n","        r2 = model.score(X, y)\n","        coef = model.coef_[0]\n","        intercept = model.intercept_\n","        results[col] = {\n","            'formula': f\"${col}_top3 = {coef:.4f} \\cdot {col}_avg3 + {intercept:.4f}$\",\n","            'r2': r2\n","        }\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":420,"status":"ok","timestamp":1723708479918,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"JQXye8ogD_ka"},"outputs":[],"source":["# make polinomial regression\n","\n","def regressione_polinomiale(df1, df2, degree):\n","    merged_df = allinea_dataframes(df1, df2)\n","    results = {}\n","    for col in ['swim_time', 'bike_time', 'run_time', 'total_time']:\n","        polynomial_features = PolynomialFeatures(degree=degree)\n","        model = make_pipeline(polynomial_features, LinearRegression())\n","        X = merged_df[[f'{col}_avg3']].values.reshape(-1, 1)\n","        y = merged_df[f'{col}_top3'].values\n","        model.fit(X, y)\n","        r2 = model.score(X, y)\n","\n","        # find the coefficients and intercept of the polynomial model\n","        coeffs = model.named_steps['linearregression'].coef_\n","        intercept = model.named_steps['linearregression'].intercept_\n","\n","        # LaTeX formula\n","        terms = [f\"{coeff:.4f}x^{i}\" for i, coeff in enumerate(coeffs)]\n","        formula = f\"${col}_top3 = {intercept:.4f} + \" + \" + \".join(terms) + \"$\"\n","\n","        results[col] = {\n","            'formula': formula,\n","            'r2': r2\n","        }\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708479918,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"OJ4wqe2xGauq"},"outputs":[],"source":["# create heat map\n","\n","def plot_heatmap_ttest(ttest_results):\n","    # Create DataFrame for T-statistic and p-values\n","    t_stat_df = pd.DataFrame({col: [ttest_results[col]['t_statistic']] for col in ttest_results})\n","    p_value_df = pd.DataFrame({col: [ttest_results[col]['p_value']] for col in ttest_results})\n","\n","    # Plot heatmap for T-statistic\n","    plt.figure(figsize=(10, 2))\n","    sns.heatmap(t_stat_df, annot=True, cmap='coolwarm', cbar=True, fmt=\".4f\")\n","    plt.title(f'Heatmap for T-test {sex}')\n","    plt.show()\n","\n","    # Plot heatmap for P-values\n","    plt.figure(figsize=(10, 2))\n","    sns.heatmap(p_value_df, annot=True, cmap='coolwarm', cbar=True, fmt=\".8f\")\n","    plt.title(f'Heatmap for P-values {sex}')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723708479918,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"aQLTh3VRe_GG"},"outputs":[],"source":["# finction to covert coluimns in type number\n","def ensure_numeric(df, columns):\n","    for col in columns:\n","        df[col] = pd.to_numeric(df[col], errors='coerce')\n","    return df\n","\n","columns_to_check = ['swim_time', 'bike_time', 'run_time', 'total_time']\n","\n","# convert columns in type number\n","train_df_avg3 = ensure_numeric(train_df_avg3, columns_to_check)\n","train_df_top3 = ensure_numeric(train_df_top3, columns_to_check)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1723708480290,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"ST4MZYNfDHiU","outputId":"a4ed0e9c-2ca4-4318-c0a5-715c74c2aac3"},"outputs":[],"source":["ttest_results = calcola_ttest_latex(train_df_avg3, train_df_top3)\n","plot_heatmap_ttest(ttest_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1723708480511,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"s2mtzgo1D2uE","outputId":"41fb441b-8937-497e-bfa8-3e1376a8d29c"},"outputs":[],"source":["df1 = train_df_top3\n","df2 = train_df_avg3\n","\n","# t-test on total_time\n","t_stat, p_value = ttest_ind(df1['total_time'], df2['total_time'], equal_var=False)\n","\n","# result interpretation\n","print(f\"T-test: {t_stat}\")\n","print(f\"P-value: {p_value}\")\n","\n","if p_value < 0.05:\n","    print(\"The average of 'total_time' between two datasets is significantly different.\")\n","else:\n","    print(\"The average of 'total_time' between two dataset is not significantly different.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"executionInfo":{"elapsed":1185,"status":"ok","timestamp":1723708481692,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"DWI3YqzGGamq","outputId":"cf004349-2780-4a36-f34f-cd529fbb55dc"},"outputs":[],"source":["def plot_heatmap_rquadro(r2_results):\n","    # make a dataframe for R^2\n","    r2_df = pd.DataFrame(r2_results, index=[0])\n","\n","    # Plot heatmap for R^2\n","    plt.figure(figsize=(10, 2))\n","    sns.heatmap(r2_df, annot=True, cmap='coolwarm', cbar=True, fmt=\".4f\")\n","    plt.title(f'Heatmap for R-square {sex}')\n","    plt.show()\n","\n","r2_results = calcola_rquadro_latex(train_df_avg3, train_df_top3)\n","plot_heatmap_rquadro(r2_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":794},"executionInfo":{"elapsed":2418,"status":"ok","timestamp":1723708484107,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"ceMmSfjPD_iL","outputId":"7ace0db3-b769-44ac-cb05-b036e6a02f95"},"outputs":[],"source":["# make calculation\n","\n","merged_df = allinea_dataframes(train_df_avg3, train_df_top3)\n","corr = calcola_correlazione(train_df_avg3[['swim_time', 'bike_time', 'run_time', 'total_time']],\n","                            train_df_top3[['swim_time', 'bike_time', 'run_time', 'total_time']],)\n","ttest_results = calcola_ttest_latex(train_df_avg3[['swim_time', 'bike_time', 'run_time', 'total_time']],\n","                              train_df_top3[['swim_time', 'bike_time', 'run_time', 'total_time']],)\n","r2 = calcola_rquadro_latex(train_df_avg3, train_df_top3)\n","plot_heatmap_correlazione(train_df_avg3, train_df_top3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456,"status":"ok","timestamp":1723708484561,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"moXl6tQjD_ft","outputId":"bbbd5433-f68b-46f1-ae4a-bc32f677ef4c"},"outputs":[],"source":["# linear regression and polynomial regression\n","lin_reg_results = regressione_lineare(train_df_avg3, train_df_top3)\n","\n","poly_reg_results = regressione_polinomiale(train_df_avg3, train_df_top3, degree=3)\n","\n","lin_reg_results\n","poly_reg_results\n"]},{"cell_type":"markdown","metadata":{"id":"ju7yUjknTr9K"},"source":["# Position analysis\n","The pourpose of this analysis is that the winner of the race sometime had the best time in swim, bike or run.\n","\n","We want to analyze the difference between finel position and swim, bike and run position"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723708484561,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"icwrT84amquO","outputId":"89505702-3ec2-4dd5-e868-fa81c9e00ef7"},"outputs":[],"source":["df_filtered"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1723708484833,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"KRrCBRnZf22U","outputId":"8352d6f3-8049-402a-f5c6-be015ebd9560"},"outputs":[],"source":["df_filtered.dropna()\n","df_filtered"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"elapsed":18244,"status":"ok","timestamp":1723708503075,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"8FeEUQU7OR8V","outputId":"151d6029-e268-452e-86cd-9e04c87185a1"},"outputs":[],"source":["def plot_positions_with_regression(df):\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n","\n","    # chart 1: final position Vs. swim position with confidence interval and a line of regression\n","    # change the confident interval as your use\n","    sns.regplot(\n","        x='position',\n","        y='swim_position',\n","        data=df,\n","        ax=axes[0],\n","        ci=95,\n","        marker=\".\",\n","        scatter_kws={'color': 'blue', 'alpha': 0.3},\n","        line_kws={'color': 'black', 'lw': 2},\n","        scatter=True\n","    )\n","    axes[0].set_title(f'Final Position Vs. Swim Position {sex}')\n","    axes[0].set_xlabel('Final Position')\n","    axes[0].set_ylabel('Swim Position')\n","\n","    # chart 2: final position Vs. bike position with confidence interval and a line of regression\n","    sns.regplot(\n","        x='position',\n","        y='bike_position',\n","        data=df,\n","        ax=axes[1],\n","        ci=95,\n","        marker=\".\",\n","        scatter_kws={'color': 'green', 'alpha': 0.3},\n","        line_kws={'color': 'black', 'lw': 2},\n","        scatter=True\n","    )\n","    axes[1].set_title(f'Final Position Vs. Bike Position {sex}')\n","    axes[1].set_xlabel('Final Position')\n","    axes[1].set_ylabel('Bike Position')\n","\n","    # chart 3: final position Vs. run position with confidence interval and a line of regression\n","    sns.regplot(\n","        x='position',\n","        y='run_position',\n","        data=df,\n","        ax=axes[2],\n","        ci=95,\n","        marker=\".\",\n","        scatter_kws={'color': 'red', 'alpha': 0.3},\n","        line_kws={'color': 'black', 'lw': 2},\n","        scatter=True\n","    )\n","    axes[2].set_title(f'Final Position Vs. Run Position {sex}')\n","    axes[2].set_xlabel('Final Position ')\n","    axes[2].set_ylabel('Run Position')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_positions_with_regression(df_filtered)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":546,"status":"ok","timestamp":1723708503619,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"NnO7YAmdPuS_","outputId":"46ea64f0-556a-44b9-9a50-fabd061e5cc8"},"outputs":[],"source":["def regression(df, degree=[1, 5]):\n","    results = {}\n","\n","    # Define the independent variable\n","    X = df['position'].values.reshape(-1, 1)\n","\n","    # Define the dependent variables\n","    y_vars = ['swim_position', 'bike_position', 'run_position']\n","\n","    # Iterate over each dependent variable\n","    for y_var in y_vars:\n","        y = df[y_var].values\n","        results[y_var] = {}\n","\n","        # Iterate over the range of degrees\n","        for deg in range(degree[0], degree[1] + 1):\n","            # Polynomial transformation for degrees > 1\n","            if deg > 1:\n","                poly = PolynomialFeatures(degree=deg)\n","                X_poly = poly.fit_transform(X)\n","                model = LinearRegression()\n","                model.fit(X_poly, y)\n","                y_pred = model.predict(X_poly)\n","                intercept = model.intercept_\n","                coefs = model.coef_\n","                r_squared = r2_score(y, y_pred)\n","\n","                # Create the polynomial equation string\n","                equation = f\"{y_var} = {intercept:.4f}\"\n","                terms = [f\"{coefs[i]:+.4f}*x^{i}\" for i in range(1, len(coefs))]\n","                equation += \" \" + \" \".join(terms)\n","            else:\n","                # Linear regression (deg=1)\n","                model = LinearRegression()\n","                model.fit(X, y)\n","                y_pred = model.predict(X)\n","                intercept = model.intercept_\n","                slope = model.coef_[0]\n","                r_squared = r2_score(y, y_pred)\n","                equation = f\"{y_var} = {intercept:.4f} + {slope:.4f}*x\"\n","\n","            # Store results for this variable and degree\n","            results[y_var][f\"degree_{deg}\"] = {\n","                'equation': equation,\n","                'r_squared': r_squared\n","            }\n","\n","    return results\n","\n","results = regression(df_filtered, degree=[1, 5])\n","\n","# Print the results\n","for y_var, degrees in results.items():\n","    print(f\"Results for {y_var}:\")\n","    for deg, info in degrees.items():\n","        print(f\"  {deg}: {info['equation']} (R^2 = {info['r_squared']:.4f})\")\n","    print(\"\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1723708503619,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"fG_P4UIPOR3F"},"outputs":[],"source":["def get_winners_positions(df):\n","    # filtering for winners only\n","    winners = df[df['position'] == 1]\n","    return winners[['swim_position', 'bike_position', 'run_position']]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1723708503619,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"b_vfMRanOR0k"},"outputs":[],"source":["def descriptive_statistics(winners):\n","    stats = winners.describe()\n","    return stats"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1723708503619,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"e-IJ7XFnORx8"},"outputs":[],"source":["def plot_winners_positions(winners):\n","    plt.figure(figsize=(12, 6))\n","    sns.boxplot(data=winners)\n","    plt.title(f\"Distribution of the discipline position for winner {sex}\")\n","    plt.xlabel('Discipline')\n","    plt.ylabel('Position')\n","    plt.ylim(0, 30)\n","    plt.yticks(np.arange(1, 30 + 1, 1))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1723708503619,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"Sp6YRnhQRyKk"},"outputs":[],"source":["def compare_disciplines(winners):\n","    swim_positions = winners['swim_position']\n","    bike_positions = winners['bike_position']\n","    run_positions = winners['run_position']\n","\n","    # Friedman test to check the position distribution\n","    stat, p_value = friedmanchisquare(swim_positions, bike_positions, run_positions)\n","\n","    return stat, p_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1723708503620,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"qUq0D-AAR6JO","outputId":"b5566fa1-03b6-4faf-d765-7875a65b928e"},"outputs":[],"source":["winners_positions = get_winners_positions(df_filtered)\n","\n","statistics = descriptive_statistics(winners_positions)\n","print(\"Statistiche Descrittive:\\n\", statistics)\n","\n","plot_winners_positions(winners_positions)\n","\n","stat, p_value = compare_disciplines(winners_positions)\n","print(f\"Test di Friedman: stat={stat}, p_value={p_value}\")\n","\n","if p_value < 0.01:\n","    print(\"Interpretation: Very Good - There is a highly significant difference between the disciplines.\")\n","elif 0.01 <= p_value < 0.05:\n","    print(\"Interpretation: Good - There is a significant difference between the disciplines.\")\n","elif 0.05 <= p_value < 0.10:\n","    print(\"Interpretation: Moderate - There is a marginal difference between the disciplines.\")\n","elif 0.10 <= p_value < 0.20:\n","    print(\"Interpretation: Poor - There is little to no significant difference between the disciplines.\")\n","else:\n","    print(\"Interpretation: Very Poor - There is no significant difference between the disciplines.\")\n"]},{"cell_type":"markdown","metadata":{"id":"lfrURghvQ8Ek"},"source":["# Interpretation Scale for P-Value\n","Very Good (Highly Significant Difference)\n","\n","## p-value: Less than 0.01\n","- Interpretation: There is a very strong evidence that the rankings across the groups differ. This suggests that the performance in different disciplines (e.g., swimming, cycling, running) has a significant and consistent impact on the outcome.\n","- Example: \"The Friedman test indicates a highly significant difference between the disciplines, suggesting that at least one discipline is consistently more influential in determining winners' positions.\"\n","Good (Significant Difference)\n","\n","## p-value: Between 0.01 and 0.05\n","- Interpretation: There is strong evidence that the rankings across the groups differ. This suggests that differences between disciplines are likely meaningful and affect the outcome.\n","- Example: \"The test shows a significant difference between the disciplines, implying that performance in at least one discipline likely contributes more to the overall result.\"\n","Moderate (Marginal Difference)\n","\n","## p-value: Between 0.05 and 0.10\n","- Interpretation: There is some evidence that the rankings across the groups differ, but the difference is not as strong. The influence of different disciplines may be present but less pronounced.\n","- Example: \"There is a marginal difference between disciplines, indicating that their impact on the outcome might be there, but it is not as strong.\"\n","Poor (Little to No Difference)\n","\n","## p-value: Between 0.10 and 0.20\n","- Interpretation: There is weak evidence for differences between the groups. The performance in different disciplines may have little to no significant effect on the outcome.\n","- Example: \"The results suggest little difference between disciplines, implying that no single discipline has a significant impact on the overall outcome.\"\n","Very Poor (No Significant Difference)\n","\n","## p-value: Greater than 0.20\n","- Interpretation: There is no significant evidence that the rankings differ between groups. The disciplines likely do not have a meaningful impact on the overall result.\n","- Example: \"The Friedman test shows no significant difference between the disciplines, suggesting that each discipline contributes similarly to the final result.\""]},{"cell_type":"markdown","metadata":{"id":"NoBwT5TxQElJ"},"source":["# Definition of the Friedman Test\n","\n","The Friedman test is a non-parametric statistical test used to detect differences in treatments across multiple test attempts. It is an alternative to the repeated-measures ANOVA when the assumption of normality is violated. The test is particularly useful when you have three or more related samples, such as measurements taken on the same subjects under different conditions.\n","\n","### Key Points:\n","\n","- **Purpose**: The Friedman test evaluates whether there are significant differences between the groups based on ranks.\n","- **Data Type**: It works with ordinal data or non-normally distributed continuous data.\n","- **Assumptions**: The test assumes that the data is paired and that there is no interaction between the groups.\n","\n","### Interpreting the Friedman Test:\n","\n","- **p-value**: The primary output of the Friedman test is the p-value.\n","  - If the **p-value** is **less than 0.05**, it suggests that there are statistically significant differences between the groups.\n","  - If the **p-value** is **greater than 0.05**, it suggests that there are no statistically significant differences between the groups.\n","\n","- **Chi-Square Statistic (Q)**: The Friedman test also provides a chi-square statistic.\n","  - The larger the **Q** value, the greater the likelihood that the observed ranks differ significantly from what would be expected if there were no differences between the groups.\n","\n","### Approximate Values for a \"Good\" Result:\n","\n","- **p-value < 0.05**: Indicates a significant difference between groups, which might be considered \"good\" if you're looking for evidence that the treatments or conditions are not equal.\n","- **Effect Size (if reported)**: An effect size (e.g., Kendall's W) closer to 1 indicates a stronger effect, meaning the differences between groups are more pronounced.\n","\n","In summary, a \"good\" result from the Friedman test, indicating significant differences between your conditions or groups, would typically involve a p-value less than 0.05 and a high chi-square statistic."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723708503620,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"Gn8UbU2OS_FH"},"outputs":[],"source":["def get_positions_in_range(df, start_position, end_position):\n","    # Filtra il DataFrame per ottenere solo le posizioni finali all'interno dell'intervallo specificato\n","    filtered_positions = df[(df['position'] >= start_position) & (df['position'] <= end_position)]\n","    return filtered_positions[['swim_position', 'bike_position', 'run_position']]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723708503620,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"MmjBvsJbS_C9"},"outputs":[],"source":["def descriptive_statistics(positions):\n","    stats = positions.describe()\n","    return stats"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":243,"status":"ok","timestamp":1723710708314,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"pef5moDGS_Am"},"outputs":[],"source":["def plot_positions_in_range(positions, start_position, end_position):\n","    plt.figure(figsize=(12, 6))\n","    sns.boxplot(data=positions)\n","    plt.title(f'Distribution of the discipline position for final position {start_position}-{end_position} {sex}')\n","    plt.ylim(0, 40)\n","    plt.xlabel('Discipline')\n","    plt.ylabel('Position')\n","    plt.yticks(np.arange(1, 40 + 1, 1))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1723708503961,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"r5VVL0agS--V"},"outputs":[],"source":["def compare_disciplines(positions):\n","    swim_positions = positions['swim_position']\n","    bike_positions = positions['bike_position']\n","    run_positions = positions['run_position']\n","\n","    # Friedman test to campare the discipline\n","    stat, p_value = friedmanchisquare(swim_positions, bike_positions, run_positions)\n","\n","    if p_value < 0.01:\n","        print(\"Interpretation: Very Good - There is a highly significant difference between the disciplines.\")\n","    elif 0.01 <= p_value < 0.05:\n","        print(\"Interpretation: Good - There is a significant difference between the disciplines.\")\n","    elif 0.05 <= p_value < 0.10:\n","        print(\"Interpretation: Moderate - There is a marginal difference between the disciplines.\")\n","    elif 0.10 <= p_value < 0.20:\n","        print(\"Interpretation: Poor - There is little to no significant difference between the disciplines.\")\n","    else:\n","        print(\"Interpretation: Very Poor - There is no significant difference between the disciplines.\")\n","\n","    return stat, p_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1723708503961,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"9Bpqs8IgS-8I"},"outputs":[],"source":["def generate_descriptive_sentence(metric, value, percentage):\n","    return f\"Position {metric} is {int(value)}, and {percentage:.2f}% of the athlete obtain this position\"\n","\n","def calculate_percentages(positions, value):\n","    # calculate the percentage of athletes who have that position.\n","    return (positions == value).mean() * 100\n","\n","def calculate_above_50th_percentile(positions, median_value):\n","    # count how many athlete have over 50 percentile\n","    above_50th = positions[positions > median_value].count()\n","    total = positions.count()\n","    percentage = (above_50th / total) * 100\n","    return above_50th, percentage\n","\n","def analyze_positions_range(df, start_position, end_position):\n","\n","    positions_in_range = get_positions_in_range(df, start_position, end_position)\n","\n","    statistics = descriptive_statistics(positions_in_range)\n","    print(\"Descriptive statistic:\\n\", statistics)\n","\n","    plot_positions_in_range(positions_in_range, start_position, end_position)\n","\n","    stat, p_value = compare_disciplines(positions_in_range)\n","    print(f\"Friedman Test: stat={stat}, p_value={p_value}\")\n","\n","    if p_value < 0.01:\n","        print(\"Interpretation: Very Good - There is a highly significant difference between the disciplines.\")\n","    elif 0.01 <= p_value < 0.05:\n","        print(\"Interpretation: Good - There is a significant difference between the disciplines.\")\n","    elif 0.05 <= p_value < 0.10:\n","        print(\"Interpretation: Moderate - There is a marginal difference between the disciplines.\")\n","    elif 0.10 <= p_value < 0.20:\n","        print(\"Interpretation: Poor - There is little to no significant difference between the disciplines.\")\n","    else:\n","        print(\"Interpretation: Very Poor - There is no significant difference between the disciplines.\")\n","\n","    for col in ['swim_position', 'bike_position', 'run_position']:\n","        print(f\"\\nDescriptive {col}:\")\n","        for metric in ['min', '25%', '50%', '75%', 'max', 'mean']:\n","            value = statistics.at[metric, col]\n","            if metric == 'mean':\n","                value = round(value)\n","            percentage = calculate_percentages(positions_in_range[col], value)\n","            sentence = generate_descriptive_sentence(metric, value, percentage)\n","            print(sentence)\n","\n","        median_value = round(statistics.at['50%', col])\n","        above_50th, percentage_above_50th = calculate_above_50th_percentile(positions_in_range[col], median_value)\n","        print(f\"The position over 50% have {above_50th} athlete and are {percentage_above_50th:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2269,"status":"ok","timestamp":1723710714287,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"sxHY0pFnS-6B","outputId":"faf8fa25-93a3-4fee-b1cb-0f9b841232f0"},"outputs":[],"source":["# comment/uncomment to select range (valid for each analysis)\n","\n","#analyze_positions_range(df_filtered, 1, 1)\n","#analyze_positions_range(df_filtered, 1, 3)\n","analyze_positions_range(df_filtered, 4, 10)\n"]},{"cell_type":"markdown","metadata":{"id":"cfC78C0Um_7W"},"source":["## We want to understand how many athelte win the race and reach the first position in swim, bike and run"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723708505417,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"UE0vC1bGoYoU","outputId":"6480d85d-8e34-4924-9dd1-09eee2d14321"},"outputs":[],"source":["train_df = train_df.loc[~(train_df == 0).any(axis=1)]\n","train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1723708505831,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"qu1FSYhNmij-","outputId":"b74b53e4-9269-4b68-ea9f-7fab3ac149b0"},"outputs":[],"source":["# Convert column in int to have correct data\n","train_df['position'] = train_df['position'].astype(int)\n","train_df['swim_position'] = train_df['swim_position'].astype(int)\n","train_df['bike_position'] = train_df['bike_position'].astype(int)\n","train_df['run_position'] = train_df['run_position'].astype(int)\n","\n","# filering row for 1 position in swim, bike, run\n","filtered_df = train_df[(train_df['position'] == 1) &\n","                       (train_df['swim_position'] == 1) &\n","                       (train_df['bike_position'] == 1) &\n","                       (train_df['run_position'] == 1)]\n","\n","# count number of row for that condition\n","count = filtered_df.shape[0]\n","\n","percentage_overall = round((count/unique_race)*100, 2)\n","\n","\n","print(f\"Number of times the athlete who finished 1st overall also finished 1st in swim, bike, and run: {count}\\n and this is the {percentage_overall}% of the race considered for {sex}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":586},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1723708506243,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"Sj1ByuuvsPMa","outputId":"4417ba9c-4911-431e-b96a-1e21c739d3e7"},"outputs":[],"source":["# make interval from 1989 to 2024\n","full_year_range = pd.Series(range(1989, 2025))\n","\n","# numebr of event for missing data\n","events_per_year = filtered_df['event_date'].dt.year.value_counts().reindex(full_year_range, fill_value=0).sort_index()\n","\n","# chart cration\n","plt.figure(figsize=(12, 6))\n","\n","# horizontalline black\n","plt.hlines(y=0, xmin=1989, xmax=2024, color='black', linewidth=2)\n","\n","# vertical line\n","for year, count in events_per_year.items():\n","    plt.vlines(x=year, ymin=0, ymax=count, color='red', linewidth=2)\n","    plt.text(year, count + 0.1, str(count), ha='center', va='bottom', fontsize=11, color='black')\n","\n","# set x and y axis\n","plt.xticks(full_year_range, rotation=90)\n","plt.yticks(range(0, events_per_year.max() + 1))\n","plt.xlabel('Year')\n","plt.ylabel('Number of races')\n","plt.title(f'Number of winners with overall swim, bike, run (Female)')\n","plt.grid(False)\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1723708506244,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"NPgbRNskloxd","outputId":"65ab7493-66e1-4cbd-d754-83202c1c6afc"},"outputs":[],"source":["filtered_df.head(10)"]},{"cell_type":"markdown","metadata":{"id":"TroS-fdGcZX3"},"source":["## Determining the most influent part of the race using classic statistic with correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1723709337293,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"tGpXNxqrS-3V","outputId":"d0d95006-d260-4b26-efd5-7131036f28cd"},"outputs":[],"source":["def analyze_influence_statistically(df, start_position, end_position):\n","    # filetering data by position range\n","    filtered_df = df[(df['position'] >= start_position) & (df['position'] <= end_position)]\n","\n","    #  calculate statistics for each discipline (swim, bike, run)\n","    swim_stats = filtered_df['swim_time'].describe()\n","    bike_stats = filtered_df['bike_time'].describe()\n","    run_stats = filtered_df['run_time'].describe()\n","\n","    # calculate correlation between final position and each discipline (swim, bike, run)\n","    correlations = filtered_df[['swim_time', 'bike_time', 'run_time', 'position']].corr()['position'][:-1]\n","\n","    # print result\n","    print(f\"Statistical Analysis from {start_position} to {end_position}:\\n\")\n","    print(\"Descriptive statistic for swim_time:\\n\", swim_stats)\n","    print(\"\\nDescriptive statistic for bike_time:\\n\", bike_stats)\n","    print(\"\\nDescriptive statistic for run_time:\\n\", run_stats)\n","    print(\"\\nCorrelation for the final position:\\n\", correlations)\n","\n","    # determine the most influenced discipline\n","    most_influential_corr = correlations.idxmin()\n","    print(f\"\\nThe most influential discipline for this position range is: {most_influential_corr}\")\n","\n","\n","analyze_influence_statistically(df_filtered, 1, 1)\n","#analyze_influence_statistically(df_filtered, 1, 3)\n","#analyze_influence_statistically(df_filtered, 4, 10)\n"]},{"cell_type":"markdown","metadata":{"id":"bxs7vtsDcg6p"},"source":["# Determining the most influent pert of the race using classic statistic with linear regression"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1723709378511,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"ib-lWzTucANH","outputId":"e3e227e6-8ab6-4b96-fa25-8b00308cd80f"},"outputs":[],"source":["def analyze_influence_statistically(df, start_position, end_position):\n","    # Filter the data based on the specified position range\n","    filtered_df = df[(df['position'] >= start_position) & (df['position'] <= end_position)]\n","\n","    # Define the independent variables (swim_time, bike_time, run_time) and the dependent variable (position)\n","    X = filtered_df[['swim_time', 'bike_time', 'run_time']]\n","    y = filtered_df['position']\n","\n","    # Fit a linear regression model\n","    model = LinearRegression()\n","    model.fit(X, y)\n","\n","    # Get the coefficients for each discipline\n","    coefficients = pd.Series(model.coef_, index=X.columns)\n","\n","    # Print the results\n","    print(f\"Statistical analysis using Linear Regression for final positions between {start_position} and {end_position}:\\n\")\n","    print(\"Linear Regression Coefficients for each discipline:\\n\", round(coefficients, 7))\n","\n","    # Determine the most influential discipline\n","    most_influential_linear = coefficients.idxmin()  # Lower coefficient (more negative) indicates a stronger influence\n","    print(f\"\\nThe most influential discipline for this position range is: {most_influential_linear}\")\n","\n","\n","#analyze_influence_statistically(df_filtered, 1, 1)\n","analyze_influence_statistically(df_filtered, 1, 3)\n","#analyze_influence_statistically(df_filtered, 4, 10)\n"]},{"cell_type":"markdown","metadata":{"id":"3eaO7FZicuR5"},"source":["# Determining the most influent pert of the race using classic statistic with polynomial regression"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1723709412394,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"YVWWXfPVcRUX","outputId":"64044770-3c1e-4c2b-bbaa-4b8661ba02e6"},"outputs":[],"source":["def analyze_influence_statistically(df, start_position, end_position, degree=2):\n","    # Filter the data based on the specified position range\n","    filtered_df = df[(df['position'] >= start_position) & (df['position'] <= end_position)]\n","\n","    # Define the independent variables (swim_time, bike_time, run_time) and the dependent variable (position)\n","    X = filtered_df[['swim_time', 'bike_time', 'run_time']]\n","    y = filtered_df['position']\n","\n","    # Apply polynomial features to the independent variables\n","    poly = PolynomialFeatures(degree=degree, include_bias=False)\n","    X_poly = poly.fit_transform(X)\n","\n","    # Fit a linear regression model on the polynomial features\n","    model = LinearRegression()\n","    model.fit(X_poly, y)\n","\n","    # Extract the coefficients (for interpretation, use the original feature names)\n","    feature_names = poly.get_feature_names_out(X.columns)\n","    coefficients = pd.Series(model.coef_, index=feature_names)\n","\n","    # Print the results\n","    print(f\"Statistical analysis using Polynomial Regression (degree={degree}) for final positions between {start_position} and {end_position}:\\n\")\n","    print(\"Polynomial Regression Coefficients for each discipline:\\n\", coefficients)\n","\n","    # Determine the most influential discipline\n","    # Sum coefficients for each original discipline feature\n","    influence = {\n","        'swim_time': coefficients.filter(like='swim_time').sum(),\n","        'bike_time': coefficients.filter(like='bike_time').sum(),\n","        'run_time': coefficients.filter(like='run_time').sum()\n","    }\n","    most_influential = min(influence, key=influence.get)\n","\n","    print(f\"\\nThe most influential discipline for this position range is: {most_influential}\")\n","    print(f\"\\nTotal influence: {influence}\")\n","\n","\n","#analyze_influence_statistically(df_filtered, 1, 1, degree=2)\n","analyze_influence_statistically(df_filtered, 1, 3, degree=2)\n","#analyze_influence_statistically(df_filtered, 4, 10, degree=2)\n"]},{"cell_type":"markdown","metadata":{"id":"sovjLiqvcxPo"},"source":["# Determining the most influent part of the race using machine learning (sklearn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":850,"status":"ok","timestamp":1723709451673,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"yDngVLZyZK-h","outputId":"de2c78bf-d172-4ba9-a927-1364d7b456ca"},"outputs":[],"source":["def analyze_influence_ml(df, start_position, end_position):\n","    # Create a target column: 1 if the final position is within the specified range, otherwise 0\n","    df['target'] = np.where((df['position'] >= start_position) & (df['position'] <= end_position), 1, 0)\n","\n","    # Select the features and the target\n","    X = df[['swim_time', 'bike_time', 'run_time']]\n","    y = df['target']\n","\n","    # Split the dataset into training and test sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","    # Train the logistic regression model\n","    model = LogisticRegression(max_iter=1000)\n","    model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = model.predict(X_test)\n","\n","    # Evaluate the model\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    # Handle case with only one class in y_test\n","    labels = np.unique(y_test)\n","    target_names = ['Other Positions', 'Top Positions'][:len(labels)]\n","\n","    # Generate the classification report\n","    report = classification_report(y_test, y_pred, target_names=target_names, labels=labels, output_dict=True)\n","\n","    # Extract precision, recall, and f1-score for the positive class (Top Positions)\n","    precision = report[target_names[-1]]['precision'] if len(labels) > 1 else \"NaN\"\n","    recall = report[target_names[-1]]['recall'] if len(labels) > 1 else \"NaN\"\n","    f1_score = report[target_names[-1]]['f1-score'] if len(labels) > 1 else \"NaN\"\n","\n","    # Extract coefficients for each discipline\n","    coefficients = pd.Series(model.coef_[0], index=X.columns)\n","\n","    # Determine the most influential discipline\n","    most_influential = coefficients.idxmin()\n","\n","    # Return metrics and most influential discipline\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1_score,\n","        'most_influential': most_influential\n","    }\n","\n","#ml_results = analyze_influence_ml(df_filtered, 1, 1)\n","ml_results = analyze_influence_ml(df_filtered, 1, 3)\n","#ml_results = analyze_influence_ml(df_filtered, 4, 10)\n","\n","# Print the results\n","print(f\"Model Accuracy: {ml_results['accuracy']}\")\n","print(f\"Model Precision: {ml_results['precision']}\")\n","print(f\"Model Recall: {ml_results['recall']}\")\n","print(f\"Model F1-Score: {ml_results['f1_score']}\")\n","print(f\"Most Influential Discipline: {ml_results['most_influential']}\")\n"]},{"cell_type":"markdown","metadata":{"id":"A7nC7JU4c13_"},"source":["# Determining the most influent part of the race using machine learning (tensorflow)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142056,"status":"ok","timestamp":1723709618987,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"6nd4GPCRZK8O","outputId":"c06e99c5-0981-490c-f4b5-5c23682a5db2"},"outputs":[],"source":["def analyze_influence_tensorflow(df, start_position, end_position):\n","    # Create a target column: 1 if the final position is within the specified range, otherwise 0\n","    df['target'] = np.where((df['position'] >= start_position) & (df['position'] <= end_position), 1, 0)\n","\n","    # Select the features and the target\n","    X = df[['swim_time', 'bike_time', 'run_time']].values\n","    y = df['target'].values\n","\n","    # Standardize the features\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    # Split the dataset into training and test sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","    # Create the TensorFlow model\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n","        tf.keras.layers.Dense(16, activation='relu'),\n","        tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","\n","    # Compile the model\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    # Train the model\n","    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n","\n","    # Evaluate the model\n","    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n","\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    # Handle case with only one class in y_test\n","    labels = np.unique(y_test)\n","    target_names = ['Other Positions', 'Top Positions'][:len(labels)]\n","\n","    report = classification_report(y_test, y_pred, target_names=target_names, labels=labels, output_dict=True)\n","\n","    precision = report[target_names[-1]]['precision'] if len(labels) > 1 else \"NaN\"\n","    recall = report[target_names[-1]]['recall'] if len(labels) > 1 else \"NaN\"\n","    f1_score = report[target_names[-1]]['f1-score'] if len(labels) > 1 else \"NaN\"\n","\n","    # Extract feature weights to understand the influence of each discipline\n","    weights = model.layers[0].get_weights()[0].flatten()\n","    influence = dict(zip(['swim_time', 'bike_time', 'run_time'], weights))\n","    most_influential = min(influence, key=influence.get)\n","\n","    # Print results (optional, can be removed if you only want to return the data)\n","    print(f\"TensorFlow Model Analysis for final positions between {start_position} and {end_position}:\\n\")\n","    print(f\"Model Accuracy: {accuracy:.4f}\")\n","    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","    print(f\"Feature Weights: {influence}\")\n","    print(f\"\\nThe most influential discipline for this position range is: {most_influential}\")\n","\n","    # Return metrics and most influential discipline\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1_score,\n","        'most_influential': most_influential\n","    }\n","\n","\n","#tf_results = analyze_influence_tensorflow(df_filtered, 1, 1)\n","tf_results = analyze_influence_tensorflow(df_filtered, 1, 3)\n","#tf_results = analyze_influence_tensorflow(df_filtered, 4, 10)\n","\n","# Print the results\n","print(f\"Model Accuracy: {tf_results['accuracy']}\")\n","print(f\"Model Precision: {tf_results['precision']}\")\n","print(f\"Model Recall: {tf_results['recall']}\")\n","print(f\"Model F1-Score: {tf_results['f1_score']}\")\n","print(f\"Most Influential Discipline: {tf_results['most_influential']}\")\n"]},{"cell_type":"markdown","metadata":{"id":"3teFLrg7bCeB"},"source":["### Model Evaluation Metrics\n","\n","1. **Precision**:\n","   - **Definition**: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive by the model.\n","   - **Formula**: `Precision = TP / (TP + FP)`\n","   - **Interpretation**: High precision means that most of the positive predictions made by the model are correct (few false positives).\n","\n","2. **Recall (Sensitivity or True Positive Rate)**:\n","   - **Definition**: Recall measures the proportion of actual positives that were correctly identified by the model.\n","   - **Formula**: `Recall = TP / (TP + FN)`\n","   - **Interpretation**: High recall indicates that the model is good at identifying most of the actual positives (few false negatives).\n","\n","3. **F1-Score**:\n","   - **Definition**: The F1-score is the harmonic mean of precision and recall, providing a balanced measure that considers both false positives and false negatives.\n","   - **Formula**: `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n","   - **Interpretation**: The F1-score is useful when it is important to balance precision and recall, especially in situations with imbalanced data.\n","\n","4. **Support**:\n","   - **Definition**: Support indicates the total number of occurrences of each class in the test data.\n","   - **Interpretation**: It shows the distribution of classes in the dataset, helping to understand the basis for calculating the metrics.\n","\n","5. **Accuracy**:\n","   - **Definition**: Accuracy measures the proportion of correctly predicted instances out of the total predictions.\n","   - **Formula**: `Accuracy = (TP + TN) / (TP + TN + FP + FN)`\n","   - **Interpretation**: Accuracy indicates how correct the model is overall in its predictions.\n","\n","6. **Macro Avg (Macro Average)**:\n","   - **Definition**: The macro average calculates the simple average of precision, recall, and F1-score for each class, without considering the support of each class.\n","   - **Interpretation**: It provides a general measure of the model's effectiveness across all classes, treating each class equally.\n","\n","7. **Weighted Avg (Weighted Average)**:\n","   - **Definition**: The weighted average calculates the average of precision, recall, and F1-score for each class, weighted by the support of each class.\n","   - **Interpretation**: It gives an overall measure that accounts for classes with more examples, better reflecting the model's performance on an imbalanced dataset.\n","\n","### Discipline Weighting\n","\n","- **Definition**: Discipline weighting is an assessment of the relative influence of each discipline (`swim_time`, `bike_time`, `run_time`) on the final classification result.\n","\n","- **Procedure**:\n","  - In the neural network model created with TensorFlow, the weights associated with the features (`swim_time`, `bike_time`, `run_time`) in the first dense layer indicate how much each feature contributes to the final result.\n","  - After training the model, the weights are extracted. A more negative weight for a feature suggests that a lower value of that feature (e.g., a shorter time) is associated with a better final result (e.g., a top 3 position).\n","  - **Conclusion**: The discipline with the most negative weight is considered the most influential in determining a good final result because variations in that discipline have a greater impact on the likelihood of achieving a good position."]},{"cell_type":"markdown","metadata":{"id":"tz-UcHz6fgqq"},"source":["# Determining the most influent pert of the race using machine learning (PyTorch)\n","## These models, when applied to your dataset, can offer deep insights into the factors influencing athletes' final positions, providing a more comprehensive analysis than linear or polynomial regression alone.\n","- Feedforward Neural Network: Good for capturing general nonlinear relationships.\n","- Convolutional Neural Network (CNN): Useful for exploring spatial relationships in structured data.\n","- Recurrent Neural Network (RNN) / LSTM: Best for capturing temporal or sequential dependencies, especially if you have time-series data for the disciplines.\n"]},{"cell_type":"markdown","metadata":{"id":"UNc_hz2jfzLK"},"source":["## Pythorch polynomial regression"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3143,"status":"ok","timestamp":1723709691809,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"lQNwemm3ZK52","outputId":"53501233-210e-4f4c-a3f8-84909ba04bc5"},"outputs":[],"source":["class PolynomialRegressionModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(PolynomialRegressionModel, self).__init__()\n","        self.linear = nn.Linear(input_dim, 1)\n","\n","    def forward(self, x):\n","        out = self.linear(x)\n","        return out\n","\n","def analyze_influence_pytorch(df, start_position, end_position, degree=2, epochs=1000, lr=0.01):\n","    # Filter the data based on the specified position range\n","    filtered_df = df[(df['position'] >= start_position) & (df['position'] <= end_position)]\n","\n","    # Define the independent variables (swim_time, bike_time, run_time) and the dependent variable (position)\n","    X = filtered_df[['swim_time', 'bike_time', 'run_time']].values\n","    y = filtered_df['position'].values.reshape(-1, 1)\n","\n","    # Apply polynomial features to the independent variables\n","    poly = PolynomialFeatures(degree=degree, include_bias=False)\n","    X_poly = poly.fit_transform(X)\n","\n","    # Standardize the features\n","    scaler = StandardScaler()\n","    X_poly = scaler.fit_transform(X_poly)\n","\n","    # Convert data to PyTorch tensors\n","    X_tensor = torch.tensor(X_poly, dtype=torch.float32)\n","    y_tensor = torch.tensor(y, dtype=torch.float32)\n","\n","    # Initialize the model\n","    input_dim = X_poly.shape[1]\n","    model = PolynomialRegressionModel(input_dim)\n","\n","    # Define loss function and optimizer\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    # Train the model\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_tensor)\n","        loss = criterion(outputs, y_tensor)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n","\n","    # Extract the coefficients\n","    with torch.no_grad():\n","        weights = model.linear.weight.numpy().flatten()\n","\n","    # Map the coefficients to the polynomial features\n","    feature_names = poly.get_feature_names_out(['swim_time', 'bike_time', 'run_time'])\n","    coefficients = pd.Series(weights, index=feature_names)\n","\n","    # Print the results\n","    print(f\"\\nStatistical analysis using Polynomial Regression (degree={degree}) with PyTorch for final positions between {start_position} and {end_position}:\\n\")\n","    print(\"Polynomial Regression Coefficients for each discipline:\\n\", coefficients)\n","\n","    # Determine the most influential discipline\n","    influence = {\n","        'swim_time': coefficients.filter(like='swim_time').sum(),\n","        'bike_time': coefficients.filter(like='bike_time').sum(),\n","        'run_time': coefficients.filter(like='run_time').sum()\n","    }\n","    most_influential = min(influence, key=influence.get)\n","\n","    print(f\"\\nThe most influential discipline for this position range is: {most_influential}\")\n","    print(f\"\\nTotal influence: {influence}\")\n","\n","\n"," #analyze_influence_pytorch(df_filtered, 1, 1, degree=2, epochs=1000, lr=0.01)\n","analyze_influence_pytorch(df_filtered, 1, 3, degree=2, epochs=1000, lr=0.01)\n","#analyze_influence_pytorch(df_filtered, 4, 10, degree=2, epochs=1000, lr=0.01)\n"]},{"cell_type":"markdown","metadata":{"id":"_6Y49N3Kf7DH"},"source":["## Pythorch Feedforward Neural Network\n","1. Feedforward Neural Network (Fully Connected Network)\n","- A basic neural network with multiple hidden layers can capture complex relationships between input features (like swim, bike, and run times) and the target variable (final position). This model is more flexible than polynomial regression as it can approximate any continuous function."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10125,"status":"ok","timestamp":1723709745894,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"0r8fVNTCgHch","outputId":"9797c222-0232-4d88-f891-470c9f78b85b"},"outputs":[],"source":["class NeuralNetworkModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(NeuralNetworkModel, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, 1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        out = torch.sigmoid(self.fc3(x))\n","        return out\n","\n","def analyze_influence_neural_network(df, start_position, end_position, epochs=1000, lr=0.01):\n","    filtered_df = df[(df['position'] >= start_position) & (df['position'] <= end_position)]\n","    X = filtered_df[['swim_time', 'bike_time', 'run_time']].values\n","    y = (filtered_df['position'] <= end_position).astype(int).values.reshape(-1, 1)\n","\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    X_tensor = torch.tensor(X, dtype=torch.float32)\n","    y_tensor = torch.tensor(y, dtype=torch.float32)\n","\n","    input_dim = X.shape[1]\n","    model = NeuralNetworkModel(input_dim)\n","\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_tensor)\n","        loss = criterion(outputs, y_tensor)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n","\n","    with torch.no_grad():\n","        y_pred = (outputs.numpy() > 0.5).astype(int)\n","        y_true = y_tensor.numpy().astype(int)\n","\n","    # Handle case with only one class in y_true\n","    labels = np.unique(y_true)\n","    target_names = ['Other Positions', 'Top Positions'][:len(labels)]\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    report = classification_report(y_true, y_pred, target_names=target_names, labels=labels, output_dict=True)\n","\n","    precision = report[target_names[-1]]['precision'] if len(labels) > 1 else \"NaN\"\n","    recall = report[target_names[-1]]['recall'] if len(labels) > 1 else \"NaN\"\n","    f1_score = report[target_names[-1]]['f1-score'] if len(labels) > 1 else \"NaN\"\n","\n","    with torch.no_grad():\n","        weights = model.fc1.weight.data.numpy().mean(axis=0)\n","\n","    influence = {\n","        'swim_time': weights[0],\n","        'bike_time': weights[1],\n","        'run_time': weights[2]\n","    }\n","    most_influential = min(influence, key=influence.get)\n","\n","    # Print results (optional)\n","    print(f\"\\nNeural Network Model Analysis for positions between {start_position} and {end_position}:\\n\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=target_names))\n","    print(f\"\\nThe most influential discipline for this position range is: {most_influential}\")\n","\n","    # Return metrics and most influential discipline\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1_score,\n","        'most_influential': most_influential\n","    }\n","\n","\n","#nn_results = analyze_influence_neural_network(df_filtered, 1, 1, epochs=1000, lr=0.01)\n","nn_results = analyze_influence_neural_network(df_filtered, 1, 3, epochs=1000, lr=0.01)\n","#nn_results = analyze_influence_neural_network(df_filtered, 4, 10, epochs=1000, lr=0.01)\n","\n","# Print the results\n","print(f\"Model Accuracy: {nn_results['accuracy']}\")\n","print(f\"Model Precision: {nn_results['precision']}\")\n","print(f\"Model Recall: {nn_results['recall']}\")\n","print(f\"Model F1-Score: {nn_results['f1_score']}\")\n","print(f\"Most Influential Discipline: {nn_results['most_influential']}\")\n"]},{"cell_type":"markdown","metadata":{"id":"c5zKa0MJf6-I"},"source":["## Pythorch Convolutional Neural Networ\n","2. Convolutional Neural Network (CNN)\n","- While CNNs are typically used for image data, they can also be applied to structured data by treating the input features as a one-dimensional \"image\" and learning spatial hierarchies in the data. This might be useful if there are complex interactions between features."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12001,"status":"ok","timestamp":1723709778541,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"_CKhaI-KgWkg","outputId":"4c12ae36-b02a-47fb-a927-ff1820761878"},"outputs":[],"source":["class CNNModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(CNNModel, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 16, kernel_size=2)\n","        self.fc1 = nn.Linear(16 * (input_dim - 1), 32)\n","        self.fc2 = nn.Linear(32, 1)\n","        self.relu = nn.ReLU()\n","        self.flatten = nn.Flatten()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.flatten(x)\n","        x = self.relu(self.fc1(x))\n","        out = torch.sigmoid(self.fc2(x))\n","        return out\n","\n","def analyze_influence_cnn(df, start_position, end_position, epochs=1000, lr=0.01):\n","    filtered_df = df[(df['position'] >= start_position) & (df['position'] <= end_position)]\n","    X = filtered_df[['swim_time', 'bike_time', 'run_time']].values\n","    y = (filtered_df['position'] <= end_position).astype(int).values.reshape(-1, 1)\n","\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # Add a dimension for Conv1d\n","    y_tensor = torch.tensor(y, dtype=torch.float32)\n","\n","    input_dim = X.shape[1]\n","    model = CNNModel(input_dim)\n","\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_tensor)\n","        loss = criterion(outputs, y_tensor)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n","\n","    with torch.no_grad():\n","        y_pred = (outputs.numpy() > 0.5).astype(int)\n","        y_true = y_tensor.numpy().astype(int)\n","\n","    # Handle case with only one class in y_true\n","    labels = np.unique(y_true)\n","    target_names = ['Other Positions', 'Top Positions'][:len(labels)]\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    report = classification_report(y_true, y_pred, target_names=target_names, labels=labels, output_dict=True)\n","\n","    precision = report[target_names[-1]]['precision'] if len(labels) > 1 else \"NaN\"\n","    recall = report[target_names[-1]]['recall'] if len(labels) > 1 else \"NaN\"\n","    f1_score = report[target_names[-1]]['f1-score'] if len(labels) > 1 else \"NaN\"\n","\n","    with torch.no_grad():\n","        filters = model.conv1.weight.data.numpy().flatten()\n","\n","    influence = {\n","        'swim_time': filters[0],\n","        'bike_time': filters[1],\n","        'run_time': filters[2]\n","    }\n","    most_influential = min(influence, key=influence.get)\n","\n","    # Print results (optional)\n","    print(f\"\\nCNN Model Analysis for positions between {start_position} and {end_position}:\\n\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=target_names))\n","    print(f\"\\nThe most influential discipline for this position range is: {most_influential}\")\n","\n","    # Return metrics and most influential discipline\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1_score,\n","        'most_influential': most_influential\n","    }\n","\n","\n","#cnn_results = analyze_influence_cnn(df_filtered, 1, 1, epochs=1000, lr=0.01)\n","cnn_results = analyze_influence_cnn(df_filtered, 1, 3, epochs=1000, lr=0.01)\n","#cnn_results = analyze_influence_cnn(df_filtered, 4, 10, epochs=1000, lr=0.01)\n","\n","# Print the results\n","print(f\"Model Accuracy: {cnn_results['accuracy']}\")\n","print(f\"Model Precision: {cnn_results['precision']}\")\n","print(f\"Model Recall: {cnn_results['recall']}\")\n","print(f\"Model F1-Score: {cnn_results['f1_score']}\")\n","print(f\"Most Influential Discipline: {cnn_results['most_influential']}\")\n"]},{"cell_type":"markdown","metadata":{"id":"br3H_5O8f65g"},"source":["## Pythorch Recurrent Neural Network (RNN) / Long Short-Term Memory (LSTM)\n","3. Recurrent Neural Network (RNN) / Long Short-Term Memory (LSTM)\n","- If the times for swimming, biking, and running are collected over multiple intervals (e.g., splits within the disciplines), an RNN or LSTM could capture the sequential dependencies between these intervals and how they influence the final position."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21272,"status":"ok","timestamp":1723709817194,"user":{"displayName":"luca bianchini","userId":"05106305922201525069"},"user_tz":-120},"id":"_Q_MEOZngfbh","outputId":"bece1e95-205a-410c-9510-6a63ce530f87"},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, 32, batch_first=True)\n","        self.fc = nn.Linear(32, 1)\n","\n","    def forward(self, x):\n","        h_0 = torch.zeros(1, x.size(0), 32).requires_grad_()\n","        c_0 = torch.zeros(1, x.size(0), 32).requires_grad_()\n","\n","        output, (h_out, _) = self.lstm(x, (h_0, c_0))\n","        h_out = h_out.view(-1, 32)\n","        out = torch.sigmoid(self.fc(h_out))\n","        return out\n","\n","def analyze_influence_lstm(df, start_position, end_position, epochs=1000, lr=0.01):\n","    filtered_df = df[(df['position'] >= start_position) & (df['position'] <= end_position)]\n","    X = filtered_df[['swim_time', 'bike_time', 'run_time']].values\n","    y = (filtered_df['position'] <= end_position).astype(int).values.reshape(-1, 1)\n","\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # Add a dimension for LSTM\n","    y_tensor = torch.tensor(y, dtype=torch.float32)\n","\n","    input_dim = X.shape[1]\n","    model = LSTMModel(input_dim)\n","\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_tensor)\n","        loss = criterion(outputs, y_tensor)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (epoch+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n","\n","    with torch.no_grad():\n","        y_pred = (outputs.numpy() > 0.5).astype(int)\n","        y_true = y_tensor.numpy().astype(int)\n","\n","    # Handle case with only one class in y_true\n","    labels = np.unique(y_true)\n","    target_names = ['Other Positions', 'Top Positions'][:len(labels)]\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    report = classification_report(y_true, y_pred, target_names=target_names, labels=labels, output_dict=True)\n","\n","    precision = report[target_names[-1]]['precision'] if len(labels) > 1.1 else \"NaN\"\n","    recall = report[target_names[-1]]['recall'] if len(labels) > 1.1 else \"NaN\"\n","    f1_score = report[target_names[-1]]['f1-score'] if len(labels) > 1.1 else \"NaN\"\n","\n","    with torch.no_grad():\n","        weights = model.fc.weight.data.numpy().flatten()\n","\n","    influence = {\n","        'swim_time': weights[0],\n","        'bike_time': weights[1],\n","        'run_time': weights[2]\n","    }\n","    most_influential = min(influence, key=influence.get)\n","\n","    # Print results (optional)\n","    print(f\"\\nLSTM Model Analysis for positions between {start_position} and {end_position}:\\n\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=target_names))\n","    print(f\"\\nThe most influential discipline for this position range is: {most_influential}\")\n","\n","    # Return metrics and most influential discipline\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1_score,\n","        'most_influential': most_influential\n","    }\n","\n","\n","#lstm_results = analyze_influence_lstm(df_filtered, 1, 1, epochs=1000, lr=0.01)\n","lstm_results = analyze_influence_lstm(df_filtered, 1, 3, epochs=1000, lr=0.01)\n","#lstm_results = analyze_influence_lstm(df_filtered, 4, 10, epochs=1000, lr=0.01)\n","\n","# Print the results\n","print(f\"Model Accuracy: {lstm_results['accuracy']}\")\n","print(f\"Model Precision: {lstm_results['precision']}\")\n","print(f\"Model Recall: {lstm_results['recall']}\")\n","print(f\"Model F1-Score: {lstm_results['f1_score']}\")\n","print(f\"Most Influential Discipline: {lstm_results['most_influential']}\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMGP5BdKSFSxEY6VHArMdMH","mount_file_id":"1-bEMCDcdUT0BsddBMMKrgOB2s-78XGfU","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
